import time
import os 
from IPython.display import SVG
from keras.utils.vis_utils import model_to_dot
import matplotlib.pyplot as plt 
os.chdir(r'c:/users/vasil/desktop/parsing_data')
import pandas as pd
import numpy as np
from operator import itemgetter
from nltk.corpus import stopwords
from nltk.tokenize import word_tokenize
from wordcloud import WordCloud
import string
import pymorphy2
morph = pymorphy2.MorphAnalyzer()
from sklearn.model_selection import train_test_split
from sklearn.metrics import f1_score
from sklearn.metrics import classification_report
from keras.preprocessing.text import Tokenizer
from keras.preprocessing.sequence import pad_sequences
from keras.models import Sequential
from keras.layers import Input
from keras.layers import Dense, Flatten, Embedding, GlobalMaxPool1D, Dropout,SpatialDropout1D, LSTM, Bidirectional, Conv1D
from keras.layers.convolutional import MaxPooling1D
from keras.optimizers import Adam
from keras.layers.recurrent import GRU
from keras.models import load_model
from keras.callbacks import ReduceLROnPlateau, EarlyStopping, ModelCheckpoint
from sklearn.metrics import roc_auc_score
import pickle



data = pd.read_csv('clean_data_1.csv', sep=';', encoding='windows-1251')
data = data.drop('Unnamed: 0', axis=1)
labels = data.loc[:,'salary':'court_threat'].astype('category')
data['text'] = data['text'].map(lambda x: clear(str(x)))
num_labels = 7

tokenizer = Tokenizer(num_words=5000, lower=True)
tokenizer.fit_on_texts(data['text'])
vocab_size = len(tokenizer.word_index)+1

counts = []
for category in list(data.columns.values)[1:]:
    counts.append(data[category].sum())
  
class_weights={
    0: 1.0/counts[0],
    1: 1.0/counts[1],
    2: 1.0/counts[2],
    3: 1.0/counts[3],
    4: 1.0/counts[4],
    5: 1.0/counts[5],
    6: 1.0/counts[6],
}

x_train, x_test,y_train, y_test = train_test_split(data['text'], labels, random_state=42, test_size=0.2, shuffle=True)

sequences_train = tokenizer.texts_to_sequences(x_train)
train_x = np.array(pad_sequences(sequences_train, maxlen=90))

sequences_train_1 = tokenizer.texts_to_sequences(x_test)
test_x = np.array(pad_sequences(sequences_train_1, maxlen=90))

epochs = 100
batch_size = 64
n_words_to_skip = 50
pad_type = trunc_type = 'pre'
drop_embed = 0.2
n_dense = 256
dropout = 0.2
n_conv = 256
k_conv = 3
embedding_dim = 300

model = Sequential()
model.add(Embedding(vocab_size, embedding_dim,input_length = 90, trainable=True))
model.add(Dropout(0.2))
model.add(LSTM(50))
model.add(Dense(7,activation='sigmoid'))


# компиляция модели
model.compile(optimizer=Adam(0.015), loss='binary_crossentropy', metrics=['categorical_accuracy'])
callbacks = [
    ReduceLROnPlateau(),
    EarlyStopping(patience=4), 
    ModelCheckpoint(filepath='Convolutional.h5', save_best_only=True)
]
# обучение модели
history = model.fit(train_x, y_train, class_weight=class_weights, batch_size=batch_size, epochs=epochs, 
    verbose=1, validation_split=.1,
    callbacks=callbacks)

#_-------сохранение модели
# creates a HDF5 file 'my_model.h5'
model.model.save('working_model.h5')
 
# Save Tokenizer i.e. Vocabulary
with open('tokenizer.pickle', 'wb') as handle:
    pickle.dump(tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)
    
    
# пример
inp_txt = 'Начальник дурак и зарплата маленькая'
clean_inp = clear(inp_txt)
acc = history.history['categorical_accuracy']
val_acc = history.history['val_categorical_accuracy']
loss = history.history['loss']
val_loss = history.history['val_loss']

epochs = range(1, len(acc) + 1)

plt.plot(epochs, acc, 'b', label='Training categ acc')
plt.plot(epochs, val_acc, 'b', color='red', label='Validation categ acc')
plt.title('Training and validation categorical accuracy')
plt.legend()

plt.figure()

plt.plot(epochs, loss, 'b',  label='Training loss')
plt.plot(epochs, val_loss, 'b',color='red', label='Validation loss')
plt.title('Training and validation loss')
plt.legend()

plt.show()
textArray = np.array(pad_sequences(tokenizer.texts_to_sequences([clean_inp]), maxlen=90))

    
