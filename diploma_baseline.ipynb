{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import re\n",
    "from wordcloud import WordCloud,STOPWORDS\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "import seaborn as sns\n",
    "os.chdir(r'c:/users/vasil/desktop/parsing_data')\n",
    "from nltk.tokenize import word_tokenize\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "import pymorphy2\n",
    "morph = pymorphy2.MorphAnalyzer()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def clear(txt):\n",
    "    tokens=word_tokenize(txt)\n",
    "    tokens=[morph.parse(w)[0].normal_form for w in tokens]\n",
    "    tbl=str.maketrans('', '',string.punctuation)\n",
    "    stripped=[w.translate(tbl) for w in tokens]\n",
    "    stop_words = stopwords.words('russian')                                                               \n",
    "    words=[word for word in stripped if word.isalpha() and word not in stop_words] \n",
    "    return str(words)\n",
    "\n",
    "#vectorizer = TfidfVectorizer(strip_accents='unicode', analyzer='word', ngram_range=(1,3), norm='l2')\n",
    "\n",
    "data = pd.read_csv('clean_data_1.csv', sep=';', encoding='windows-1251')\n",
    "data = data.drop('Unnamed: 0', axis=1)\n",
    "labels = data.loc[:,'salary':'court_threat'].astype('category')\n",
    "data['text'] = data['text'].map(lambda x: clear(str(x)))\n",
    "num_labels = 7\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train, test = train_test_split(data['text'], labels, random_state=42, test_size=0.20, shuffle=True)\n",
    "train, test = train_test_split(data,random_state=42, test_size=0.2, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "X_train = train.text\n",
    "X_test = test.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      salary       0.10      0.08      0.09        24\n",
      "        boss       0.26      0.31      0.28        32\n",
      "  atmosphere       0.41      0.49      0.45        35\n",
      " fire_threat       0.21      0.30      0.25        10\n",
      "   work_cond       0.51      0.56      0.54        34\n",
      "     neutral       0.88      0.62      0.73        24\n",
      "court_threat       0.08      0.12      0.10         8\n",
      "\n",
      " avg / total       0.40      0.40      0.39       167\n",
      "\n",
      "accuracy_score:  0.2358\n",
      "macro_avg f1-score:  0.3481\n",
      "micro_avg f1-score:  0.3862\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\metrics\\classification.py:1428: UserWarning: labels size, 7, does not match size of target_names, 612\n",
      "  .format(len(labels), len(target_names))\n"
     ]
    }
   ],
   "source": [
    "# using binary relevance\n",
    "from skmultilearn.problem_transform import BinaryRelevance\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "#https://towardsdatascience.com/multi-class-text-classification-model-comparison-and-selection-5eb066197568\n",
    "\n",
    "SVC_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stopwords.words('russian'))),\n",
    "                ('clf', BinaryRelevance(LinearSVC())),\n",
    "            ])\n",
    "\n",
    "NB_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stopwords.words('russian'))),\n",
    "                ('clf', BinaryRelevance(GaussianNB(\n",
    "                    ))),\n",
    "            ])\n",
    "LogReg_pipeline = Pipeline([\n",
    "               ('tfidf', TfidfVectorizer(stop_words=stopwords.words('russian'))),\n",
    "               ('clf', BinaryRelevance(LogisticRegression(C=1500))),\n",
    "           ])\n",
    "\n",
    "sgd_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stopwords.words('russian'))),\n",
    "                ('clf', BinaryRelevance(SGDClassifier(loss='hinge', penalty='l2',\n",
    "                    alpha=1e-3, random_state=42, max_iter=5, tol=None))),\n",
    "               ])\n",
    "\n",
    "NB_pipeline.fit(X_train, train.loc[:,'salary':'court_threat'])\n",
    "prediction = NB_pipeline.predict(X_test)\n",
    "print(classification_report(test.loc[:,'salary':'court_threat'], prediction,target_names=labels))\n",
    "print('accuracy_score: ', round(accuracy_score(test.loc[:,'salary':'court_threat'], prediction),4))\n",
    "print('macro_avg f1-score: ', round(f1_score(test.loc[:,'salary':'court_threat'], prediction, average='macro'),4))\n",
    "print('micro_avg f1-score: ', round(f1_score(test.loc[:,'salary':'court_threat'], prediction, average='micro'),4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      salary       0.92      0.46      0.61        24\n",
      "        boss       0.76      0.50      0.60        32\n",
      "  atmosphere       0.48      0.40      0.44        35\n",
      " fire_threat       0.44      0.40      0.42        10\n",
      "   work_cond       0.67      0.41      0.51        34\n",
      "     neutral       0.50      0.79      0.61        24\n",
      "court_threat       0.33      0.12      0.18         8\n",
      "\n",
      " avg / total       0.63      0.47      0.52       167\n",
      "\n",
      "accuracy_score:  0.3821\n",
      "macro_avg f1-score:  0.4825\n",
      "micro_avg f1-score:  0.5267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\metrics\\classification.py:1428: UserWarning: labels size, 7, does not match size of target_names, 612\n",
      "  .format(len(labels), len(target_names))\n"
     ]
    }
   ],
   "source": [
    "from skmultilearn.problem_transform import LabelPowerset\n",
    "\n",
    "LogReg_pipeline = Pipeline([\n",
    "               ('tfidf', TfidfVectorizer(stop_words=stopwords.words('russian'))),\n",
    "               ('clf', LabelPowerset(LogisticRegression(penalty = 'l2', C = 1500,random_state = 0))),\n",
    "           ])\n",
    "NB_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stopwords.words('russian'))),\n",
    "                ('clf', LabelPowerset(GaussianNB(\n",
    "                    ))),\n",
    "            ])\n",
    "sgd_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stopwords.words('russian'))),\n",
    "                ('clf', BinaryRelevance(SGDClassifier(loss='hinge', penalty='l2',\n",
    "                    alpha=1e-3, random_state=42, max_iter=5, tol=None))),\n",
    "               ])\n",
    "\n",
    "\n",
    "LogReg_pipeline.fit(X_train, train.loc[:,'salary':'court_threat'])\n",
    "prediction = LogReg_pipeline .predict(X_test)\n",
    "print(classification_report(test.loc[:,'salary':'court_threat'], prediction,target_names=labels))\n",
    "print('accuracy_score: ', round(accuracy_score(test.loc[:,'salary':'court_threat'], prediction),4))\n",
    "print('macro_avg f1-score: ', round(f1_score(test.loc[:,'salary':'court_threat'], prediction, average='macro'),4))\n",
    "print('micro_avg f1-score: ', round(f1_score(test.loc[:,'salary':'court_threat'], prediction, average='micro'),4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2032520325203252\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "      salary       0.50      0.04      0.08        24\n",
      "        boss       0.50      0.12      0.20        32\n",
      "  atmosphere       0.67      0.06      0.11        35\n",
      " fire_threat       0.67      0.20      0.31        10\n",
      "   work_cond       0.33      0.03      0.05        34\n",
      "     neutral       0.20      0.92      0.33        24\n",
      "court_threat       0.00      0.00      0.00         8\n",
      "\n",
      " avg / total       0.44      0.19      0.15       167\n",
      "\n",
      "macro_avg f1-score:  0.1535\n",
      "micro_avg f1-score:  0.2162\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\metrics\\classification.py:1428: UserWarning: labels size, 7, does not match size of target_names, 612\n",
      "  .format(len(labels), len(target_names))\n"
     ]
    }
   ],
   "source": [
    "from skmultilearn.adapt import MLkNN\n",
    "from scipy.sparse import csr_matrix, lil_matrix\n",
    "\n",
    "vectorizer = TfidfVectorizer()\n",
    "vectorizer.fit(train.text)\n",
    "vectorizer.fit(test.text)\n",
    "X_train = vectorizer.transform(train.text)\n",
    "X_test = vectorizer.transform(test.text)\n",
    "\n",
    "\n",
    "x_train = lil_matrix(X_train).toarray()\n",
    "y_train = lil_matrix(train.loc[:,'salary':'court_threat']).toarray()\n",
    "x_test = lil_matrix(X_test).toarray()\n",
    "\n",
    "\"\"\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'k': [1,2,3,4,5]}\n",
    "mlknn = MLkNN()\n",
    "gs_clf_svm = GridSearchCV(mlknn, parameters, scoring='f1_micro')\n",
    "gs_clf_svm = gs_clf_svm.fit(X_train,y_train)\n",
    "\n",
    "print(gs_clf_svm.best_score_)\n",
    "print(gs_clf_svm.best_params_)\n",
    "\"\"\"\n",
    "mlknn = MLkNN(2)\n",
    "mlknn.fit(X_train,y_train)\n",
    "prediction = mlknn.predict(x_test)\n",
    "print(accuracy_score(test.loc[:,'salary':'court_threat'], prediction))\n",
    "print(classification_report(test.loc[:,'salary':'court_threat'], prediction,target_names=labels))\n",
    "print('macro_avg f1-score: ', round(f1_score(test.loc[:,'salary':'court_threat'], prediction, average='macro'),4))\n",
    "print('micro_avg f1-score: ', round(f1_score(test.loc[:,'salary':'court_threat'], prediction, average='micro'),4))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2764227642276423\n"
     ]
    }
   ],
   "source": [
    "SVC_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stopwords.words('russian'))),\n",
    "                ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n",
    "            ])\n",
    "\n",
    "NB_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stopwords.words('russian'))),\n",
    "                ('clf', OneVsRestClassifier(MultinomialNB(\n",
    "                    fit_prior=True, class_prior=None))),\n",
    "            ])\n",
    "SVC_pipeline.fit(X_train,train.loc[:,'salary':'court_threat'])\n",
    "\n",
    "\n",
    "\n",
    "prediction = SVC_pipeline.predict(X_test)\n",
    "print(accuracy_score(test.loc[:,'salary':'court_threat'], prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "JoblibValueError",
     "evalue": "JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...\\\\dp\\\\lib\\\\site-packages\\\\ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\runpy.py in _run_code(code=<code object <module> at 0x000001D3881134B0, fil...lib\\site-packages\\ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\__pycache__\\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': r'C:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...\\\\dp\\\\lib\\\\site-packages\\\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\v...dp\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...\\\\dp\\\\lib\\\\site-packages\\\\ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x000001D3881134B0, fil...lib\\site-packages\\ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\__pycache__\\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': r'C:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...\\\\dp\\\\lib\\\\site-packages\\\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\v...dp\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\traitlets\\config\\application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\ipykernel\\kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    500         if self.poller is not None:\n    501             self.poller.start()\n    502         self.kernel.start()\n    503         self.io_loop = ioloop.IOLoop.current()\n    504         try:\n--> 505             self.io_loop.start()\n        self.io_loop.start = <bound method BaseAsyncIOLoop.start of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n    506         except KeyboardInterrupt:\n    507             pass\n    508 \n    509 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\tornado\\platform\\asyncio.py in start(self=<tornado.platform.asyncio.AsyncIOMainLoop object>)\n    127         except (RuntimeError, AssertionError):\n    128             old_loop = None\n    129         try:\n    130             self._setup_logging()\n    131             asyncio.set_event_loop(self.asyncio_loop)\n--> 132             self.asyncio_loop.run_forever()\n        self.asyncio_loop.run_forever = <bound method BaseEventLoop.run_forever of <_Win...EventLoop running=True closed=False debug=False>>\n    133         finally:\n    134             asyncio.set_event_loop(old_loop)\n    135 \n    136     def stop(self):\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\asyncio\\base_events.py in run_forever(self=<_WindowsSelectorEventLoop running=True closed=False debug=False>)\n    422             sys.set_asyncgen_hooks(firstiter=self._asyncgen_firstiter_hook,\n    423                                    finalizer=self._asyncgen_finalizer_hook)\n    424         try:\n    425             events._set_running_loop(self)\n    426             while True:\n--> 427                 self._run_once()\n        self._run_once = <bound method BaseEventLoop._run_once of <_Windo...EventLoop running=True closed=False debug=False>>\n    428                 if self._stopping:\n    429                     break\n    430         finally:\n    431             self._stopping = False\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\asyncio\\base_events.py in _run_once(self=<_WindowsSelectorEventLoop running=True closed=False debug=False>)\n   1435                         logger.warning('Executing %s took %.3f seconds',\n   1436                                        _format_handle(handle), dt)\n   1437                 finally:\n   1438                     self._current_handle = None\n   1439             else:\n-> 1440                 handle._run()\n        handle._run = <bound method Handle._run of <Handle IOLoop._run_callback(functools.par...328>, ...]))>))>>\n   1441         handle = None  # Needed to break cycles when an exception occurs.\n   1442 \n   1443     def _set_coroutine_wrapper(self, enabled):\n   1444         try:\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\asyncio\\events.py in _run(self=<Handle IOLoop._run_callback(functools.par...328>, ...]))>))>)\n    140             self._callback = None\n    141             self._args = None\n    142 \n    143     def _run(self):\n    144         try:\n--> 145             self._callback(*self._args)\n        self._callback = <bound method IOLoop._run_callback of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n        self._args = (functools.partial(<function wrap.<locals>.null_w...DFE3E0>, <zmq.sugar.fr...001D397DFE328>, ...]))>),)\n    146         except Exception as exc:\n    147             cb = _format_callback_source(self._callback, self._args)\n    148             msg = 'Exception in callback {}'.format(cb)\n    149             context = {\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\tornado\\ioloop.py in _run_callback(self=<tornado.platform.asyncio.AsyncIOMainLoop object>, callback=functools.partial(<function wrap.<locals>.null_w...DFE3E0>, <zmq.sugar.fr...001D397DFE328>, ...]))>))\n    753         \"\"\"Runs a callback with error handling.\n    754 \n    755         For use in subclasses.\n    756         \"\"\"\n    757         try:\n--> 758             ret = callback()\n        ret = undefined\n        callback = functools.partial(<function wrap.<locals>.null_w...DFE3E0>, <zmq.sugar.fr...001D397DFE328>, ...]))>)\n    759             if ret is not None:\n    760                 from tornado import gen\n    761                 # Functions that return Futures typically swallow all\n    762                 # exceptions and store them in the Future.  If a Future\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=(<Future finished result=(10, 48, <bound method.....7DFE3E0>, <zmq.sugar.fr...001D397DFE328>, ...]))>,), **kwargs={})\n    295         # Fast path when there are no active contexts.\n    296         def null_wrapper(*args, **kwargs):\n    297             try:\n    298                 current_state = _state.contexts\n    299                 _state.contexts = cap_contexts[0]\n--> 300                 return fn(*args, **kwargs)\n        args = (<Future finished result=(10, 48, <bound method.....7DFE3E0>, <zmq.sugar.fr...001D397DFE328>, ...]))>,)\n        kwargs = {}\n    301             finally:\n    302                 _state.contexts = current_state\n    303         null_wrapper._wrapped = True\n    304         return null_wrapper\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\tornado\\gen.py in inner(f=None)\n   1228             return False\n   1229         elif not self.future.done():\n   1230             def inner(f):\n   1231                 # Break a reference cycle to speed GC.\n   1232                 f = None  # noqa\n-> 1233                 self.run()\n   1234             self.io_loop.add_future(\n   1235                 self.future, inner)\n   1236             return False\n   1237         return True\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\tornado\\gen.py in run(self=<tornado.gen.Runner object>)\n   1142                         finally:\n   1143                             # Break up a reference to itself\n   1144                             # for faster GC on CPython.\n   1145                             exc_info = None\n   1146                     else:\n-> 1147                         yielded = self.gen.send(value)\n        yielded = undefined\n        self.gen.send = <built-in method send of generator object>\n        value = (10, 48, <bound method Kernel.dispatch_shell of <ipykernel.ipkernel.IPythonKernel object>>, (<zmq.eventloop.zmqstream.ZMQStream object>, [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]))\n   1148 \n   1149                     if stack_context._state.contexts is not orig_stack_contexts:\n   1150                         self.gen.throw(\n   1151                             stack_context.StackContextInconsistentError(\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\ipykernel\\kernelbase.py in process_one(self=<ipykernel.ipkernel.IPythonKernel object>, wait=True)\n    352         else:\n    353             try:\n    354                 priority, t, dispatch, args = self.msg_queue.get_nowait()\n    355             except QueueEmpty:\n    356                 return None\n--> 357         yield gen.maybe_future(dispatch(*args))\n        dispatch = <bound method Kernel.dispatch_shell of <ipykernel.ipkernel.IPythonKernel object>>\n        args = (<zmq.eventloop.zmqstream.ZMQStream object>, [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    358 \n    359     @gen.coroutine\n    360     def dispatch_queue(self):\n    361         \"\"\"Coroutine to preserve order of message handling\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\tornado\\gen.py in wrapper(*args=(<ipykernel.ipkernel.IPythonKernel object>, <zmq.eventloop.zmqstream.ZMQStream object>, [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]), **kwargs={})\n    321                 # never actually yields, which in turn allows us to\n    322                 # use \"optional\" coroutines in critical path code without\n    323                 # performance penalty for the synchronous case.\n    324                 try:\n    325                     orig_stack_contexts = stack_context._state.contexts\n--> 326                     yielded = next(result)\n        yielded = undefined\n        result = <generator object dispatch_shell>\n    327                     if stack_context._state.contexts is not orig_stack_contexts:\n    328                         yielded = _create_future()\n    329                         yielded.set_exception(\n    330                             stack_context.StackContextInconsistentError(\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\ipykernel\\kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': \"model = Pipeline([('tfidf', TfidfVectorizer(stop...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2019, 4, 26, 11, 12, 55, 367867, tzinfo=tzutc()), 'msg_id': '5768fbd3da094f2285d1c0938caf2e98', 'msg_type': 'execute_request', 'session': '19361b4e3f56426cb50186b49a66e139', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '5768fbd3da094f2285d1c0938caf2e98', 'msg_type': 'execute_request', 'parent_header': {}})\n    262             try:\n    263                 self.pre_handler_hook()\n    264             except Exception:\n    265                 self.log.debug(\"Unable to signal in pre_handler_hook:\", exc_info=True)\n    266             try:\n--> 267                 yield gen.maybe_future(handler(stream, idents, msg))\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'19361b4e3f56426cb50186b49a66e139']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': \"model = Pipeline([('tfidf', TfidfVectorizer(stop...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2019, 4, 26, 11, 12, 55, 367867, tzinfo=tzutc()), 'msg_id': '5768fbd3da094f2285d1c0938caf2e98', 'msg_type': 'execute_request', 'session': '19361b4e3f56426cb50186b49a66e139', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '5768fbd3da094f2285d1c0938caf2e98', 'msg_type': 'execute_request', 'parent_header': {}}\n    268             except Exception:\n    269                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    270             finally:\n    271                 try:\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\tornado\\gen.py in wrapper(*args=(<ipykernel.ipkernel.IPythonKernel object>, <zmq.eventloop.zmqstream.ZMQStream object>, [b'19361b4e3f56426cb50186b49a66e139'], {'buffers': [], 'content': {'allow_stdin': True, 'code': \"model = Pipeline([('tfidf', TfidfVectorizer(stop...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2019, 4, 26, 11, 12, 55, 367867, tzinfo=tzutc()), 'msg_id': '5768fbd3da094f2285d1c0938caf2e98', 'msg_type': 'execute_request', 'session': '19361b4e3f56426cb50186b49a66e139', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '5768fbd3da094f2285d1c0938caf2e98', 'msg_type': 'execute_request', 'parent_header': {}}), **kwargs={})\n    321                 # never actually yields, which in turn allows us to\n    322                 # use \"optional\" coroutines in critical path code without\n    323                 # performance penalty for the synchronous case.\n    324                 try:\n    325                     orig_stack_contexts = stack_context._state.contexts\n--> 326                     yielded = next(result)\n        yielded = undefined\n        result = <generator object execute_request>\n    327                     if stack_context._state.contexts is not orig_stack_contexts:\n    328                         yielded = _create_future()\n    329                         yielded.set_exception(\n    330                             stack_context.StackContextInconsistentError(\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\ipykernel\\kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'19361b4e3f56426cb50186b49a66e139'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': \"model = Pipeline([('tfidf', TfidfVectorizer(stop...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2019, 4, 26, 11, 12, 55, 367867, tzinfo=tzutc()), 'msg_id': '5768fbd3da094f2285d1c0938caf2e98', 'msg_type': 'execute_request', 'session': '19361b4e3f56426cb50186b49a66e139', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '5768fbd3da094f2285d1c0938caf2e98', 'msg_type': 'execute_request', 'parent_header': {}})\n    529             self._publish_execute_input(code, parent, self.execution_count)\n    530 \n    531         reply_content = yield gen.maybe_future(\n    532             self.do_execute(\n    533                 code, silent, store_history,\n--> 534                 user_expressions, allow_stdin,\n        user_expressions = {}\n        allow_stdin = True\n    535             )\n    536         )\n    537 \n    538         # Flush output before sending the reply.\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\tornado\\gen.py in wrapper(*args=(<ipykernel.ipkernel.IPythonKernel object>, \"model = Pipeline([('tfidf', TfidfVectorizer(stop...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\", False, True, {}, True), **kwargs={})\n    321                 # never actually yields, which in turn allows us to\n    322                 # use \"optional\" coroutines in critical path code without\n    323                 # performance penalty for the synchronous case.\n    324                 try:\n    325                     orig_stack_contexts = stack_context._state.contexts\n--> 326                     yielded = next(result)\n        yielded = undefined\n        result = <generator object do_execute>\n    327                     if stack_context._state.contexts is not orig_stack_contexts:\n    328                         yielded = _create_future()\n    329                         yielded.set_exception(\n    330                             stack_context.StackContextInconsistentError(\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\ipykernel\\ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=\"model = Pipeline([('tfidf', TfidfVectorizer(stop...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\", silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    289                     res = yield coro_future\n    290             else:\n    291                 # runner isn't already running,\n    292                 # make synchronous call,\n    293                 # letting shell dispatch to loop runners\n--> 294                 res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        code = \"model = Pipeline([('tfidf', TfidfVectorizer(stop...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\"\n        store_history = True\n        silent = False\n    295         finally:\n    296             self._restore_input()\n    297 \n    298         if res.error_before_exec is not None:\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\ipykernel\\zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=(\"model = Pipeline([('tfidf', TfidfVectorizer(stop...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\",), **kwargs={'silent': False, 'store_history': True})\n    531             )\n    532         self.payload_manager.write_payload(payload)\n    533 \n    534     def run_cell(self, *args, **kwargs):\n    535         self._last_traceback = None\n--> 536         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = (\"model = Pipeline([('tfidf', TfidfVectorizer(stop...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\",)\n        kwargs = {'silent': False, 'store_history': True}\n    537 \n    538     def _showtraceback(self, etype, evalue, stb):\n    539         # try to preserve ordering of tracebacks and print statements\n    540         sys.stdout.flush()\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=\"model = Pipeline([('tfidf', TfidfVectorizer(stop...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\", store_history=True, silent=False, shell_futures=True)\n   2814         result : :class:`ExecutionResult`\n   2815         \"\"\"\n   2816         result = None\n   2817         try:\n   2818             result = self._run_cell(\n-> 2819                 raw_cell, store_history, silent, shell_futures)\n        raw_cell = \"model = Pipeline([('tfidf', TfidfVectorizer(stop...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\"\n        store_history = True\n        silent = False\n        shell_futures = True\n   2820         finally:\n   2821             self.events.trigger('post_execute')\n   2822             if not silent:\n   2823                 self.events.trigger('post_run_cell', result)\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\IPython\\core\\interactiveshell.py in _run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=\"model = Pipeline([('tfidf', TfidfVectorizer(stop...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\", store_history=True, silent=False, shell_futures=True)\n   2840             runner = self.loop_runner\n   2841         else:\n   2842             runner = _pseudo_sync_runner\n   2843 \n   2844         try:\n-> 2845             return runner(coro)\n        runner = <function _pseudo_sync_runner>\n        coro = <generator object InteractiveShell.run_cell_async>\n   2846         except BaseException as e:\n   2847             info = ExecutionInfo(raw_cell, store_history, silent, shell_futures)\n   2848             result = ExecutionResult(info)\n   2849             result.error_in_exec = e\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\IPython\\core\\async_helpers.py in _pseudo_sync_runner(coro=<generator object InteractiveShell.run_cell_async>)\n     62 \n     63     Credit to Nathaniel Smith\n     64 \n     65     \"\"\"\n     66     try:\n---> 67         coro.send(None)\n        coro.send = <built-in method send of generator object>\n     68     except StopIteration as exc:\n     69         return exc.value\n     70     else:\n     71         # TODO: do not raise but return an execution result with the right info.\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_cell_async(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=\"model = Pipeline([('tfidf', TfidfVectorizer(stop...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\", store_history=True, silent=False, shell_futures=True)\n   3015                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   3016                 if _run_async:\n   3017                     interactivity = 'async'\n   3018 \n   3019                 has_raised = yield from self.run_ast_nodes(code_ast.body, cell_name,\n-> 3020                        interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   3021 \n   3022                 self.last_execution_succeeded = not has_raised\n   3023                 self.last_execution_result = result\n   3024 \n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>, <_ast.ImportFrom object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>, <_ast.Expr object>], cell_name='<ipython-input-33-185ea2a96147>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 1d396c671d0, executio...rue silent=False shell_futures=True> result=None>)\n   3180                     return True\n   3181             else:\n   3182                 for i, node in enumerate(to_run_exec):\n   3183                     mod = ast.Module([node])\n   3184                     code = compiler(mod, cell_name, \"exec\")\n-> 3185                     if (yield from self.run_code(code, result)):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x000001D399CCB9C0, file \"<ipython-input-33-185ea2a96147>\", line 11>\n        result = <ExecutionResult object at 1d396c671d0, executio...rue silent=False shell_futures=True> result=None>\n   3186                         return True\n   3187 \n   3188                 for i, node in enumerate(to_run_interactive):\n   3189                     mod = ast.Interactive([node])\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x000001D399CCB9C0, file \"<ipython-input-33-185ea2a96147>\", line 11>, result=<ExecutionResult object at 1d396c671d0, executio...rue silent=False shell_futures=True> result=None>, async_=False)\n   3262                 if async_:\n   3263                     last_expr = (yield from self._async_exec(code_obj, self.user_ns))\n   3264                     code = compile('last_expr', 'fake', \"single\")\n   3265                     exec(code, {'last_expr': last_expr})\n   3266                 else:\n-> 3267                     exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x000001D399CCB9C0, file \"<ipython-input-33-185ea2a96147>\", line 11>\n        self.user_global_ns = {'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'import time\\nimport os\\nimport re\\nfrom wordcloud i...mport pymorphy2\\nmorph = pymorphy2.MorphAnalyzer()', \"\\ndef clear(txt):\\n    tokens=word_tokenize(txt)\\n ...ext'].map(lambda x: clear(str(x)))\\nnum_labels = 7\", 'x_train, x_test,y_train, y_test = train_test_spl...ls, random_state=42, test_size=0.2, shuffle=True)', \"LogReg_pipeline = Pipeline([\\n                ('t...ccuracy_score(y_test[category], prediction), 4)))\", 'import time\\nimport os\\nimport re\\nfrom wordcloud i...mport pymorphy2\\nmorph = pymorphy2.MorphAnalyzer()', 'import time\\nimport os\\nimport re\\nfrom wordcloud i...mport pymorphy2\\nmorph = pymorphy2.MorphAnalyzer()', \"\\ndef clear(txt):\\n    tokens=word_tokenize(txt)\\n ...ext'].map(lambda x: clear(str(x)))\\nnum_labels = 7\", 'import time\\nimport os\\nimport re\\nfrom wordcloud i...mport pymorphy2\\nmorph = pymorphy2.MorphAnalyzer()', 'x_train, x_test,y_train, y_test = train_test_spl...ls, random_state=42, test_size=0.2, shuffle=True)', 'x_train, x_test,y_train, y_test = train_test_spl...ls, random_state=42, test_size=0.2, shuffle=True)', 'LogReg_pipeline = Pipeline([\\n                (\\'t...acy_score(y_test[category], prediction), 4)))\\n\"\"\"', 'LogReg_pipeline = Pipeline([\\n                (\\'t...acy_score(y_test[category], prediction), 4)))\\n\"\"\"', 'LogReg_pipeline = Pipeline([\\n                (\\'t...acy_score(y_test[category], prediction), 4)))\\n\"\"\"', \"\\ndef clear(txt):\\n    tokens=word_tokenize(txt)\\n ...ext'].map(lambda x: clear(str(x)))\\nnum_labels = 7\", 'x_train, x_test,y_train, y_test = train_test_spl...ls, random_state=42, test_size=0.2, shuffle=True)', \"model = Pipeline([('vectorizer', CountVectorizer...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\", \"train, test = train_test_split(data, data.oc[:,'...ls, random_state=42, test_size=0.2, shuffle=True)\", 'train, test = train_test_split(data, data.loc[:,...ls, random_state=42, test_size=0.2, shuffle=True)', 'train, test = train_test_split(data, data.loc[:,...ls, random_state=42, test_size=0.2, shuffle=True)', ...], 'LinearSVC': <class 'sklearn.svm.classes.LinearSVC'>, 'LogReg_pipeline': Pipeline(memory=None,\n     steps=[('tfidf', Tfid...bose=0, warm_start=False),\n          n_jobs=1))]), 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'MultinomialNB': <class 'sklearn.naive_bayes.MultinomialNB'>, 'OneVsRestClassifier': <class 'sklearn.multiclass.OneVsRestClassifier'>, 'Out': {30: '\\nfrom sklearn.model_selection import GridSearchC...curacy_score(y_test[category], prediction), 4)))\\n'}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'STOPWORDS': {'a', 'about', 'above', 'after', 'again', 'against', ...}, ...}\n        self.user_ns = {'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'import time\\nimport os\\nimport re\\nfrom wordcloud i...mport pymorphy2\\nmorph = pymorphy2.MorphAnalyzer()', \"\\ndef clear(txt):\\n    tokens=word_tokenize(txt)\\n ...ext'].map(lambda x: clear(str(x)))\\nnum_labels = 7\", 'x_train, x_test,y_train, y_test = train_test_spl...ls, random_state=42, test_size=0.2, shuffle=True)', \"LogReg_pipeline = Pipeline([\\n                ('t...ccuracy_score(y_test[category], prediction), 4)))\", 'import time\\nimport os\\nimport re\\nfrom wordcloud i...mport pymorphy2\\nmorph = pymorphy2.MorphAnalyzer()', 'import time\\nimport os\\nimport re\\nfrom wordcloud i...mport pymorphy2\\nmorph = pymorphy2.MorphAnalyzer()', \"\\ndef clear(txt):\\n    tokens=word_tokenize(txt)\\n ...ext'].map(lambda x: clear(str(x)))\\nnum_labels = 7\", 'import time\\nimport os\\nimport re\\nfrom wordcloud i...mport pymorphy2\\nmorph = pymorphy2.MorphAnalyzer()', 'x_train, x_test,y_train, y_test = train_test_spl...ls, random_state=42, test_size=0.2, shuffle=True)', 'x_train, x_test,y_train, y_test = train_test_spl...ls, random_state=42, test_size=0.2, shuffle=True)', 'LogReg_pipeline = Pipeline([\\n                (\\'t...acy_score(y_test[category], prediction), 4)))\\n\"\"\"', 'LogReg_pipeline = Pipeline([\\n                (\\'t...acy_score(y_test[category], prediction), 4)))\\n\"\"\"', 'LogReg_pipeline = Pipeline([\\n                (\\'t...acy_score(y_test[category], prediction), 4)))\\n\"\"\"', \"\\ndef clear(txt):\\n    tokens=word_tokenize(txt)\\n ...ext'].map(lambda x: clear(str(x)))\\nnum_labels = 7\", 'x_train, x_test,y_train, y_test = train_test_spl...ls, random_state=42, test_size=0.2, shuffle=True)', \"model = Pipeline([('vectorizer', CountVectorizer...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\", \"train, test = train_test_split(data, data.oc[:,'...ls, random_state=42, test_size=0.2, shuffle=True)\", 'train, test = train_test_split(data, data.loc[:,...ls, random_state=42, test_size=0.2, shuffle=True)', 'train, test = train_test_split(data, data.loc[:,...ls, random_state=42, test_size=0.2, shuffle=True)', ...], 'LinearSVC': <class 'sklearn.svm.classes.LinearSVC'>, 'LogReg_pipeline': Pipeline(memory=None,\n     steps=[('tfidf', Tfid...bose=0, warm_start=False),\n          n_jobs=1))]), 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'MultinomialNB': <class 'sklearn.naive_bayes.MultinomialNB'>, 'OneVsRestClassifier': <class 'sklearn.multiclass.OneVsRestClassifier'>, 'Out': {30: '\\nfrom sklearn.model_selection import GridSearchC...curacy_score(y_test[category], prediction), 4)))\\n'}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'STOPWORDS': {'a', 'about', 'above', 'after', 'again', 'against', ...}, ...}\n   3268             finally:\n   3269                 # Reset our crash handler in place\n   3270                 sys.excepthook = old_excepthook\n   3271         except SystemExit as e:\n\n...........................................................................\nc:\\users\\vasil\\desktop\\parsing_data\\<ipython-input-33-185ea2a96147> in <module>()\n      6     \n      7 from sklearn.model_selection import GridSearchCV\n      8 parameters = {'vectorizer__ngram_range': [(1, 1), (1, 2),(2,2)],\n      9                'tfidf__use_idf': (True, False)}\n     10 gs_clf_svm = GridSearchCV(model, parameters, n_jobs=-1)\n---> 11 gs_clf_svm = gs_clf_svm.fit(data['text'], data.loc[:,'salary':'court_threat'])\n     12 \n     13 print(gs_clf_svm.best_score_)\n     14 print(gs_clf_svm.best_params_)\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\model_selection\\_search.py in fit(self=GridSearchCV(cv=None, error_score='raise',\n     ...ain_score='warn',\n       scoring=None, verbose=0), X=0      ['хотеть', 'получать', 'нормальный', 'зп'..., 'крет...\nName: text, Length: 612, dtype: object, y=     salary  boss  atmosphere  fire_threat  work....0      0.0           0.0\n\n[612 rows x 7 columns], groups=None, **fit_params={})\n    634                                   return_train_score=self.return_train_score,\n    635                                   return_n_test_samples=True,\n    636                                   return_times=True, return_parameters=False,\n    637                                   error_score=self.error_score)\n    638           for parameters, (train, test) in product(candidate_params,\n--> 639                                                    cv.split(X, y, groups)))\n        cv.split = <bound method _BaseKFold.split of KFold(n_splits=3, random_state=None, shuffle=False)>\n        X = 0      ['хотеть', 'получать', 'нормальный', 'зп'..., 'крет...\nName: text, Length: 612, dtype: object\n        y =      salary  boss  atmosphere  fire_threat  work....0      0.0           0.0\n\n[612 rows x 7 columns]\n        groups = None\n    640 \n    641         # if one choose to see train score, \"out\" will contain train score info\n    642         if self.return_train_score:\n    643             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseSearchCV.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Fri Apr 26 14:12:57 2019\nPID: 17152        Python 3.6.7: C:\\Users\\vasil\\Anaconda3\\envs\\dp\\python.exe\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(memory=None,\n     steps=[('tfidf', Tfid...l=0.0001,\n     verbose=0),\n          n_jobs=1))]), 0      ['хотеть', 'получать', 'нормальный', 'зп'..., 'крет...\nName: text, Length: 612, dtype: object,      salary  boss  atmosphere  fire_threat  work....0      0.0           0.0\n\n[612 rows x 7 columns], {'score': <function _passthrough_scorer>}, array([204, 205, 206, 207, 208, 209, 210, 211, 2..., 604, 605, 606,\n       607, 608, 609, 610, 611]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...    195, 196, 197, 198, 199, 200, 201, 202, 203]), 0, {'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(memory=None,\n     steps=[('tfidf', Tfid...l=0.0001,\n     verbose=0),\n          n_jobs=1))]), 0      ['хотеть', 'получать', 'нормальный', 'зп'..., 'крет...\nName: text, Length: 612, dtype: object,      salary  boss  atmosphere  fire_threat  work....0      0.0           0.0\n\n[612 rows x 7 columns], {'score': <function _passthrough_scorer>}, array([204, 205, 206, 207, 208, 209, 210, 211, 2..., 604, 605, 606,\n       607, 608, 609, 610, 611]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...    195, 196, 197, 198, 199, 200, 201, 202, 203]), 0, {'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...l=0.0001,\n     verbose=0),\n          n_jobs=1))]), X=0      ['хотеть', 'получать', 'нормальный', 'зп'..., 'крет...\nName: text, Length: 612, dtype: object, y=     salary  boss  atmosphere  fire_threat  work....0      0.0           0.0\n\n[612 rows x 7 columns], scorer={'score': <function _passthrough_scorer>}, train=array([204, 205, 206, 207, 208, 209, 210, 211, 2..., 604, 605, 606,\n       607, 608, 609, 610, 611]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ...    195, 196, 197, 198, 199, 200, 201, 202, 203]), verbose=0, parameters={'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    439                       for k, v in fit_params.items()])\n    440 \n    441     test_scores = {}\n    442     train_scores = {}\n    443     if parameters is not None:\n--> 444         estimator.set_params(**parameters)\n        estimator.set_params = <bound method Pipeline.set_params of Pipeline(me...=0.0001,\n     verbose=0),\n          n_jobs=1))])>\n        parameters = {'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)}\n    445 \n    446     start_time = time.time()\n    447 \n    448     X_train, y_train = _safe_split(estimator, X, y, train)\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\pipeline.py in set_params(self=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...l=0.0001,\n     verbose=0),\n          n_jobs=1))]), **kwargs={'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)})\n    137 \n    138         Returns\n    139         -------\n    140         self\n    141         \"\"\"\n--> 142         self._set_params('steps', **kwargs)\n        self._set_params = <bound method _BaseComposition._set_params of Pi...=0.0001,\n     verbose=0),\n          n_jobs=1))])>\n        kwargs = {'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)}\n    143         return self\n    144 \n    145     def _validate_steps(self):\n    146         names, estimators = zip(*self.steps)\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\utils\\metaestimators.py in _set_params(self=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...l=0.0001,\n     verbose=0),\n          n_jobs=1))]), attr='steps', **params={'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)})\n     44         names, _ = zip(*getattr(self, attr))\n     45         for name in list(six.iterkeys(params)):\n     46             if '__' not in name and name in names:\n     47                 self._replace_estimator(attr, name, params.pop(name))\n     48         # 3. Step parameters and other initilisation arguments\n---> 49         super(_BaseComposition, self).set_params(**params)\n        self.set_params = <bound method Pipeline.set_params of Pipeline(me...=0.0001,\n     verbose=0),\n          n_jobs=1))])>\n        params = {'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)}\n     50         return self\n     51 \n     52     def _replace_estimator(self, attr, name, new_val):\n     53         # assumes `name` is a valid estimator name\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\base.py in set_params(self=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...l=0.0001,\n     verbose=0),\n          n_jobs=1))]), **params={'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)})\n    269             key, delim, sub_key = key.partition('__')\n    270             if key not in valid_params:\n    271                 raise ValueError('Invalid parameter %s for estimator %s. '\n    272                                  'Check the list of available parameters '\n    273                                  'with `estimator.get_params().keys()`.' %\n--> 274                                  (key, self))\n        key = 'vectorizer'\n        self = Pipeline(memory=None,\n     steps=[('tfidf', Tfid...l=0.0001,\n     verbose=0),\n          n_jobs=1))])\n    275 \n    276             if delim:\n    277                 nested_params[key][sub_key] = value\n    278             else:\n\nValueError: Invalid parameter vectorizer for estimator Pipeline(memory=None,\n     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n ...lti_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n     verbose=0),\n          n_jobs=1))]). Check the list of available parameters with `estimator.get_params().keys()`.\n___________________________________________________________________________",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRemoteTraceback\u001b[0m                           Traceback (most recent call last)",
      "\u001b[1;31mRemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 350, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 131, in __call__\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"C:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\", line 131, in <listcomp>\n    return [func(*args, **kwargs) for func, args, kwargs in self.items]\n  File \"C:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 444, in _fit_and_score\n    estimator.set_params(**parameters)\n  File \"C:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\pipeline.py\", line 142, in set_params\n    self._set_params('steps', **kwargs)\n  File \"C:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 49, in _set_params\n    super(_BaseComposition, self).set_params(**params)\n  File \"C:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\base.py\", line 274, in set_params\n    (key, self))\nValueError: Invalid parameter vectorizer for estimator Pipeline(memory=None,\n     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n ...lti_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n     verbose=0),\n          n_jobs=1))]). Check the list of available parameters with `estimator.get_params().keys()`.\n\nDuring handling of the above exception, another exception occurred:\n\nTraceback (most recent call last):\n  File \"C:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\multiprocessing\\pool.py\", line 119, in worker\n    result = (True, func(*args, **kwds))\n  File \"C:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\", line 359, in __call__\n    raise TransportableException(text, e_type)\nsklearn.externals.joblib.my_exceptions.TransportableException: TransportableException\n___________________________________________________________________________\nValueError                                         Fri Apr 26 14:12:57 2019\nPID: 17152        Python 3.6.7: C:\\Users\\vasil\\Anaconda3\\envs\\dp\\python.exe\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(memory=None,\n     steps=[('tfidf', Tfid...l=0.0001,\n     verbose=0),\n          n_jobs=1))]), 0      ['хотеть', 'получать', 'нормальный', 'зп'..., 'крет...\nName: text, Length: 612, dtype: object,      salary  boss  atmosphere  fire_threat  work....0      0.0           0.0\n\n[612 rows x 7 columns], {'score': <function _passthrough_scorer>}, array([204, 205, 206, 207, 208, 209, 210, 211, 2..., 604, 605, 606,\n       607, 608, 609, 610, 611]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...    195, 196, 197, 198, 199, 200, 201, 202, 203]), 0, {'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(memory=None,\n     steps=[('tfidf', Tfid...l=0.0001,\n     verbose=0),\n          n_jobs=1))]), 0      ['хотеть', 'получать', 'нормальный', 'зп'..., 'крет...\nName: text, Length: 612, dtype: object,      salary  boss  atmosphere  fire_threat  work....0      0.0           0.0\n\n[612 rows x 7 columns], {'score': <function _passthrough_scorer>}, array([204, 205, 206, 207, 208, 209, 210, 211, 2..., 604, 605, 606,\n       607, 608, 609, 610, 611]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...    195, 196, 197, 198, 199, 200, 201, 202, 203]), 0, {'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...l=0.0001,\n     verbose=0),\n          n_jobs=1))]), X=0      ['хотеть', 'получать', 'нормальный', 'зп'..., 'крет...\nName: text, Length: 612, dtype: object, y=     salary  boss  atmosphere  fire_threat  work....0      0.0           0.0\n\n[612 rows x 7 columns], scorer={'score': <function _passthrough_scorer>}, train=array([204, 205, 206, 207, 208, 209, 210, 211, 2..., 604, 605, 606,\n       607, 608, 609, 610, 611]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ...    195, 196, 197, 198, 199, 200, 201, 202, 203]), verbose=0, parameters={'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    439                       for k, v in fit_params.items()])\n    440 \n    441     test_scores = {}\n    442     train_scores = {}\n    443     if parameters is not None:\n--> 444         estimator.set_params(**parameters)\n        estimator.set_params = <bound method Pipeline.set_params of Pipeline(me...=0.0001,\n     verbose=0),\n          n_jobs=1))])>\n        parameters = {'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)}\n    445 \n    446     start_time = time.time()\n    447 \n    448     X_train, y_train = _safe_split(estimator, X, y, train)\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\pipeline.py in set_params(self=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...l=0.0001,\n     verbose=0),\n          n_jobs=1))]), **kwargs={'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)})\n    137 \n    138         Returns\n    139         -------\n    140         self\n    141         \"\"\"\n--> 142         self._set_params('steps', **kwargs)\n        self._set_params = <bound method _BaseComposition._set_params of Pi...=0.0001,\n     verbose=0),\n          n_jobs=1))])>\n        kwargs = {'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)}\n    143         return self\n    144 \n    145     def _validate_steps(self):\n    146         names, estimators = zip(*self.steps)\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\utils\\metaestimators.py in _set_params(self=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...l=0.0001,\n     verbose=0),\n          n_jobs=1))]), attr='steps', **params={'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)})\n     44         names, _ = zip(*getattr(self, attr))\n     45         for name in list(six.iterkeys(params)):\n     46             if '__' not in name and name in names:\n     47                 self._replace_estimator(attr, name, params.pop(name))\n     48         # 3. Step parameters and other initilisation arguments\n---> 49         super(_BaseComposition, self).set_params(**params)\n        self.set_params = <bound method Pipeline.set_params of Pipeline(me...=0.0001,\n     verbose=0),\n          n_jobs=1))])>\n        params = {'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)}\n     50         return self\n     51 \n     52     def _replace_estimator(self, attr, name, new_val):\n     53         # assumes `name` is a valid estimator name\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\base.py in set_params(self=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...l=0.0001,\n     verbose=0),\n          n_jobs=1))]), **params={'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)})\n    269             key, delim, sub_key = key.partition('__')\n    270             if key not in valid_params:\n    271                 raise ValueError('Invalid parameter %s for estimator %s. '\n    272                                  'Check the list of available parameters '\n    273                                  'with `estimator.get_params().keys()`.' %\n--> 274                                  (key, self))\n        key = 'vectorizer'\n        self = Pipeline(memory=None,\n     steps=[('tfidf', Tfid...l=0.0001,\n     verbose=0),\n          n_jobs=1))])\n    275 \n    276             if delim:\n    277                 nested_params[key][sub_key] = value\n    278             else:\n\nValueError: Invalid parameter vectorizer for estimator Pipeline(memory=None,\n     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n ...lti_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n     verbose=0),\n          n_jobs=1))]). Check the list of available parameters with `estimator.get_params().keys()`.\n___________________________________________________________________________\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mTransportableException\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m~\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    698\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dp\\lib\\multiprocessing\\pool.py\u001b[0m in \u001b[0;36mget\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    669\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 670\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    671\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTransportableException\u001b[0m: TransportableException\n___________________________________________________________________________\nValueError                                         Fri Apr 26 14:12:57 2019\nPID: 17152        Python 3.6.7: C:\\Users\\vasil\\Anaconda3\\envs\\dp\\python.exe\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(memory=None,\n     steps=[('tfidf', Tfid...l=0.0001,\n     verbose=0),\n          n_jobs=1))]), 0      ['хотеть', 'получать', 'нормальный', 'зп'..., 'крет...\nName: text, Length: 612, dtype: object,      salary  boss  atmosphere  fire_threat  work....0      0.0           0.0\n\n[612 rows x 7 columns], {'score': <function _passthrough_scorer>}, array([204, 205, 206, 207, 208, 209, 210, 211, 2..., 604, 605, 606,\n       607, 608, 609, 610, 611]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...    195, 196, 197, 198, 199, 200, 201, 202, 203]), 0, {'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(memory=None,\n     steps=[('tfidf', Tfid...l=0.0001,\n     verbose=0),\n          n_jobs=1))]), 0      ['хотеть', 'получать', 'нормальный', 'зп'..., 'крет...\nName: text, Length: 612, dtype: object,      salary  boss  atmosphere  fire_threat  work....0      0.0           0.0\n\n[612 rows x 7 columns], {'score': <function _passthrough_scorer>}, array([204, 205, 206, 207, 208, 209, 210, 211, 2..., 604, 605, 606,\n       607, 608, 609, 610, 611]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...    195, 196, 197, 198, 199, 200, 201, 202, 203]), 0, {'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...l=0.0001,\n     verbose=0),\n          n_jobs=1))]), X=0      ['хотеть', 'получать', 'нормальный', 'зп'..., 'крет...\nName: text, Length: 612, dtype: object, y=     salary  boss  atmosphere  fire_threat  work....0      0.0           0.0\n\n[612 rows x 7 columns], scorer={'score': <function _passthrough_scorer>}, train=array([204, 205, 206, 207, 208, 209, 210, 211, 2..., 604, 605, 606,\n       607, 608, 609, 610, 611]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ...    195, 196, 197, 198, 199, 200, 201, 202, 203]), verbose=0, parameters={'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    439                       for k, v in fit_params.items()])\n    440 \n    441     test_scores = {}\n    442     train_scores = {}\n    443     if parameters is not None:\n--> 444         estimator.set_params(**parameters)\n        estimator.set_params = <bound method Pipeline.set_params of Pipeline(me...=0.0001,\n     verbose=0),\n          n_jobs=1))])>\n        parameters = {'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)}\n    445 \n    446     start_time = time.time()\n    447 \n    448     X_train, y_train = _safe_split(estimator, X, y, train)\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\pipeline.py in set_params(self=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...l=0.0001,\n     verbose=0),\n          n_jobs=1))]), **kwargs={'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)})\n    137 \n    138         Returns\n    139         -------\n    140         self\n    141         \"\"\"\n--> 142         self._set_params('steps', **kwargs)\n        self._set_params = <bound method _BaseComposition._set_params of Pi...=0.0001,\n     verbose=0),\n          n_jobs=1))])>\n        kwargs = {'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)}\n    143         return self\n    144 \n    145     def _validate_steps(self):\n    146         names, estimators = zip(*self.steps)\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\utils\\metaestimators.py in _set_params(self=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...l=0.0001,\n     verbose=0),\n          n_jobs=1))]), attr='steps', **params={'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)})\n     44         names, _ = zip(*getattr(self, attr))\n     45         for name in list(six.iterkeys(params)):\n     46             if '__' not in name and name in names:\n     47                 self._replace_estimator(attr, name, params.pop(name))\n     48         # 3. Step parameters and other initilisation arguments\n---> 49         super(_BaseComposition, self).set_params(**params)\n        self.set_params = <bound method Pipeline.set_params of Pipeline(me...=0.0001,\n     verbose=0),\n          n_jobs=1))])>\n        params = {'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)}\n     50         return self\n     51 \n     52     def _replace_estimator(self, attr, name, new_val):\n     53         # assumes `name` is a valid estimator name\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\base.py in set_params(self=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...l=0.0001,\n     verbose=0),\n          n_jobs=1))]), **params={'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)})\n    269             key, delim, sub_key = key.partition('__')\n    270             if key not in valid_params:\n    271                 raise ValueError('Invalid parameter %s for estimator %s. '\n    272                                  'Check the list of available parameters '\n    273                                  'with `estimator.get_params().keys()`.' %\n--> 274                                  (key, self))\n        key = 'vectorizer'\n        self = Pipeline(memory=None,\n     steps=[('tfidf', Tfid...l=0.0001,\n     verbose=0),\n          n_jobs=1))])\n    275 \n    276             if delim:\n    277                 nested_params[key][sub_key] = value\n    278             else:\n\nValueError: Invalid parameter vectorizer for estimator Pipeline(memory=None,\n     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n ...lti_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n     verbose=0),\n          n_jobs=1))]). Check the list of available parameters with `estimator.get_params().keys()`.\n___________________________________________________________________________",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mJoblibValueError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-185ea2a96147>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m                'tfidf__use_idf': (True, False)}\n\u001b[0;32m     10\u001b[0m \u001b[0mgs_clf_svm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparameters\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mgs_clf_svm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgs_clf_svm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'text'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'salary'\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;34m'court_threat'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgs_clf_svm\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbest_score_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    637\u001b[0m                                   error_score=self.error_score)\n\u001b[0;32m    638\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[1;32m--> 639\u001b[1;33m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[0;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    641\u001b[0m         \u001b[1;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[1;31m# consumption.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    790\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    738\u001b[0m                     \u001b[0mexception\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mexception_type\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreport\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    739\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 740\u001b[1;33m                     \u001b[1;32mraise\u001b[0m \u001b[0mexception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    741\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    742\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0miterable\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mJoblibValueError\u001b[0m: JoblibValueError\n___________________________________________________________________________\nMultiprocessing exception:\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\runpy.py in _run_module_as_main(mod_name='ipykernel_launcher', alter_argv=1)\n    188         sys.exit(msg)\n    189     main_globals = sys.modules[\"__main__\"].__dict__\n    190     if alter_argv:\n    191         sys.argv[0] = mod_spec.origin\n    192     return _run_code(code, main_globals, None,\n--> 193                      \"__main__\", mod_spec)\n        mod_spec = ModuleSpec(name='ipykernel_launcher', loader=<_f...\\\\dp\\\\lib\\\\site-packages\\\\ipykernel_launcher.py')\n    194 \n    195 def run_module(mod_name, init_globals=None,\n    196                run_name=None, alter_sys=False):\n    197     \"\"\"Execute a module's code without importing it\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\runpy.py in _run_code(code=<code object <module> at 0x000001D3881134B0, fil...lib\\site-packages\\ipykernel_launcher.py\", line 5>, run_globals={'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\__pycache__\\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': r'C:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...\\\\dp\\\\lib\\\\site-packages\\\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\v...dp\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>, ...}, init_globals=None, mod_name='__main__', mod_spec=ModuleSpec(name='ipykernel_launcher', loader=<_f...\\\\dp\\\\lib\\\\site-packages\\\\ipykernel_launcher.py'), pkg_name='', script_name=None)\n     80                        __cached__ = cached,\n     81                        __doc__ = None,\n     82                        __loader__ = loader,\n     83                        __package__ = pkg_name,\n     84                        __spec__ = mod_spec)\n---> 85     exec(code, run_globals)\n        code = <code object <module> at 0x000001D3881134B0, fil...lib\\site-packages\\ipykernel_launcher.py\", line 5>\n        run_globals = {'__annotations__': {}, '__builtins__': <module 'builtins' (built-in)>, '__cached__': r'C:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\__pycache__\\ipykernel_launcher.cpython-36.pyc', '__doc__': 'Entry point for launching an IPython kernel.\\n\\nTh...orts until\\nafter removing the cwd from sys.path.\\n', '__file__': r'C:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\ipykernel_launcher.py', '__loader__': <_frozen_importlib_external.SourceFileLoader object>, '__name__': '__main__', '__package__': '', '__spec__': ModuleSpec(name='ipykernel_launcher', loader=<_f...\\\\dp\\\\lib\\\\site-packages\\\\ipykernel_launcher.py'), 'app': <module 'ipykernel.kernelapp' from 'C:\\\\Users\\\\v...dp\\\\lib\\\\site-packages\\\\ipykernel\\\\kernelapp.py'>, ...}\n     86     return run_globals\n     87 \n     88 def _run_module_code(code, init_globals=None,\n     89                     mod_name=None, mod_spec=None,\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\ipykernel_launcher.py in <module>()\n     11     # This is added back by InteractiveShellApp.init_path()\n     12     if sys.path[0] == '':\n     13         del sys.path[0]\n     14 \n     15     from ipykernel import kernelapp as app\n---> 16     app.launch_new_instance()\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\traitlets\\config\\application.py in launch_instance(cls=<class 'ipykernel.kernelapp.IPKernelApp'>, argv=None, **kwargs={})\n    653 \n    654         If a global instance already exists, this reinitializes and starts it\n    655         \"\"\"\n    656         app = cls.instance(**kwargs)\n    657         app.initialize(argv)\n--> 658         app.start()\n        app.start = <bound method IPKernelApp.start of <ipykernel.kernelapp.IPKernelApp object>>\n    659 \n    660 #-----------------------------------------------------------------------------\n    661 # utility functions, for convenience\n    662 #-----------------------------------------------------------------------------\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\ipykernel\\kernelapp.py in start(self=<ipykernel.kernelapp.IPKernelApp object>)\n    500         if self.poller is not None:\n    501             self.poller.start()\n    502         self.kernel.start()\n    503         self.io_loop = ioloop.IOLoop.current()\n    504         try:\n--> 505             self.io_loop.start()\n        self.io_loop.start = <bound method BaseAsyncIOLoop.start of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n    506         except KeyboardInterrupt:\n    507             pass\n    508 \n    509 launch_new_instance = IPKernelApp.launch_instance\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\tornado\\platform\\asyncio.py in start(self=<tornado.platform.asyncio.AsyncIOMainLoop object>)\n    127         except (RuntimeError, AssertionError):\n    128             old_loop = None\n    129         try:\n    130             self._setup_logging()\n    131             asyncio.set_event_loop(self.asyncio_loop)\n--> 132             self.asyncio_loop.run_forever()\n        self.asyncio_loop.run_forever = <bound method BaseEventLoop.run_forever of <_Win...EventLoop running=True closed=False debug=False>>\n    133         finally:\n    134             asyncio.set_event_loop(old_loop)\n    135 \n    136     def stop(self):\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\asyncio\\base_events.py in run_forever(self=<_WindowsSelectorEventLoop running=True closed=False debug=False>)\n    422             sys.set_asyncgen_hooks(firstiter=self._asyncgen_firstiter_hook,\n    423                                    finalizer=self._asyncgen_finalizer_hook)\n    424         try:\n    425             events._set_running_loop(self)\n    426             while True:\n--> 427                 self._run_once()\n        self._run_once = <bound method BaseEventLoop._run_once of <_Windo...EventLoop running=True closed=False debug=False>>\n    428                 if self._stopping:\n    429                     break\n    430         finally:\n    431             self._stopping = False\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\asyncio\\base_events.py in _run_once(self=<_WindowsSelectorEventLoop running=True closed=False debug=False>)\n   1435                         logger.warning('Executing %s took %.3f seconds',\n   1436                                        _format_handle(handle), dt)\n   1437                 finally:\n   1438                     self._current_handle = None\n   1439             else:\n-> 1440                 handle._run()\n        handle._run = <bound method Handle._run of <Handle IOLoop._run_callback(functools.par...328>, ...]))>))>>\n   1441         handle = None  # Needed to break cycles when an exception occurs.\n   1442 \n   1443     def _set_coroutine_wrapper(self, enabled):\n   1444         try:\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\asyncio\\events.py in _run(self=<Handle IOLoop._run_callback(functools.par...328>, ...]))>))>)\n    140             self._callback = None\n    141             self._args = None\n    142 \n    143     def _run(self):\n    144         try:\n--> 145             self._callback(*self._args)\n        self._callback = <bound method IOLoop._run_callback of <tornado.platform.asyncio.AsyncIOMainLoop object>>\n        self._args = (functools.partial(<function wrap.<locals>.null_w...DFE3E0>, <zmq.sugar.fr...001D397DFE328>, ...]))>),)\n    146         except Exception as exc:\n    147             cb = _format_callback_source(self._callback, self._args)\n    148             msg = 'Exception in callback {}'.format(cb)\n    149             context = {\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\tornado\\ioloop.py in _run_callback(self=<tornado.platform.asyncio.AsyncIOMainLoop object>, callback=functools.partial(<function wrap.<locals>.null_w...DFE3E0>, <zmq.sugar.fr...001D397DFE328>, ...]))>))\n    753         \"\"\"Runs a callback with error handling.\n    754 \n    755         For use in subclasses.\n    756         \"\"\"\n    757         try:\n--> 758             ret = callback()\n        ret = undefined\n        callback = functools.partial(<function wrap.<locals>.null_w...DFE3E0>, <zmq.sugar.fr...001D397DFE328>, ...]))>)\n    759             if ret is not None:\n    760                 from tornado import gen\n    761                 # Functions that return Futures typically swallow all\n    762                 # exceptions and store them in the Future.  If a Future\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\tornado\\stack_context.py in null_wrapper(*args=(<Future finished result=(10, 48, <bound method.....7DFE3E0>, <zmq.sugar.fr...001D397DFE328>, ...]))>,), **kwargs={})\n    295         # Fast path when there are no active contexts.\n    296         def null_wrapper(*args, **kwargs):\n    297             try:\n    298                 current_state = _state.contexts\n    299                 _state.contexts = cap_contexts[0]\n--> 300                 return fn(*args, **kwargs)\n        args = (<Future finished result=(10, 48, <bound method.....7DFE3E0>, <zmq.sugar.fr...001D397DFE328>, ...]))>,)\n        kwargs = {}\n    301             finally:\n    302                 _state.contexts = current_state\n    303         null_wrapper._wrapped = True\n    304         return null_wrapper\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\tornado\\gen.py in inner(f=None)\n   1228             return False\n   1229         elif not self.future.done():\n   1230             def inner(f):\n   1231                 # Break a reference cycle to speed GC.\n   1232                 f = None  # noqa\n-> 1233                 self.run()\n   1234             self.io_loop.add_future(\n   1235                 self.future, inner)\n   1236             return False\n   1237         return True\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\tornado\\gen.py in run(self=<tornado.gen.Runner object>)\n   1142                         finally:\n   1143                             # Break up a reference to itself\n   1144                             # for faster GC on CPython.\n   1145                             exc_info = None\n   1146                     else:\n-> 1147                         yielded = self.gen.send(value)\n        yielded = undefined\n        self.gen.send = <built-in method send of generator object>\n        value = (10, 48, <bound method Kernel.dispatch_shell of <ipykernel.ipkernel.IPythonKernel object>>, (<zmq.eventloop.zmqstream.ZMQStream object>, [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]))\n   1148 \n   1149                     if stack_context._state.contexts is not orig_stack_contexts:\n   1150                         self.gen.throw(\n   1151                             stack_context.StackContextInconsistentError(\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\ipykernel\\kernelbase.py in process_one(self=<ipykernel.ipkernel.IPythonKernel object>, wait=True)\n    352         else:\n    353             try:\n    354                 priority, t, dispatch, args = self.msg_queue.get_nowait()\n    355             except QueueEmpty:\n    356                 return None\n--> 357         yield gen.maybe_future(dispatch(*args))\n        dispatch = <bound method Kernel.dispatch_shell of <ipykernel.ipkernel.IPythonKernel object>>\n        args = (<zmq.eventloop.zmqstream.ZMQStream object>, [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>])\n    358 \n    359     @gen.coroutine\n    360     def dispatch_queue(self):\n    361         \"\"\"Coroutine to preserve order of message handling\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\tornado\\gen.py in wrapper(*args=(<ipykernel.ipkernel.IPythonKernel object>, <zmq.eventloop.zmqstream.ZMQStream object>, [<zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>, <zmq.sugar.frame.Frame object>]), **kwargs={})\n    321                 # never actually yields, which in turn allows us to\n    322                 # use \"optional\" coroutines in critical path code without\n    323                 # performance penalty for the synchronous case.\n    324                 try:\n    325                     orig_stack_contexts = stack_context._state.contexts\n--> 326                     yielded = next(result)\n        yielded = undefined\n        result = <generator object dispatch_shell>\n    327                     if stack_context._state.contexts is not orig_stack_contexts:\n    328                         yielded = _create_future()\n    329                         yielded.set_exception(\n    330                             stack_context.StackContextInconsistentError(\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\ipykernel\\kernelbase.py in dispatch_shell(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, msg={'buffers': [], 'content': {'allow_stdin': True, 'code': \"model = Pipeline([('tfidf', TfidfVectorizer(stop...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2019, 4, 26, 11, 12, 55, 367867, tzinfo=tzutc()), 'msg_id': '5768fbd3da094f2285d1c0938caf2e98', 'msg_type': 'execute_request', 'session': '19361b4e3f56426cb50186b49a66e139', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '5768fbd3da094f2285d1c0938caf2e98', 'msg_type': 'execute_request', 'parent_header': {}})\n    262             try:\n    263                 self.pre_handler_hook()\n    264             except Exception:\n    265                 self.log.debug(\"Unable to signal in pre_handler_hook:\", exc_info=True)\n    266             try:\n--> 267                 yield gen.maybe_future(handler(stream, idents, msg))\n        handler = <bound method Kernel.execute_request of <ipykernel.ipkernel.IPythonKernel object>>\n        stream = <zmq.eventloop.zmqstream.ZMQStream object>\n        idents = [b'19361b4e3f56426cb50186b49a66e139']\n        msg = {'buffers': [], 'content': {'allow_stdin': True, 'code': \"model = Pipeline([('tfidf', TfidfVectorizer(stop...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2019, 4, 26, 11, 12, 55, 367867, tzinfo=tzutc()), 'msg_id': '5768fbd3da094f2285d1c0938caf2e98', 'msg_type': 'execute_request', 'session': '19361b4e3f56426cb50186b49a66e139', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '5768fbd3da094f2285d1c0938caf2e98', 'msg_type': 'execute_request', 'parent_header': {}}\n    268             except Exception:\n    269                 self.log.error(\"Exception in message handler:\", exc_info=True)\n    270             finally:\n    271                 try:\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\tornado\\gen.py in wrapper(*args=(<ipykernel.ipkernel.IPythonKernel object>, <zmq.eventloop.zmqstream.ZMQStream object>, [b'19361b4e3f56426cb50186b49a66e139'], {'buffers': [], 'content': {'allow_stdin': True, 'code': \"model = Pipeline([('tfidf', TfidfVectorizer(stop...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2019, 4, 26, 11, 12, 55, 367867, tzinfo=tzutc()), 'msg_id': '5768fbd3da094f2285d1c0938caf2e98', 'msg_type': 'execute_request', 'session': '19361b4e3f56426cb50186b49a66e139', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '5768fbd3da094f2285d1c0938caf2e98', 'msg_type': 'execute_request', 'parent_header': {}}), **kwargs={})\n    321                 # never actually yields, which in turn allows us to\n    322                 # use \"optional\" coroutines in critical path code without\n    323                 # performance penalty for the synchronous case.\n    324                 try:\n    325                     orig_stack_contexts = stack_context._state.contexts\n--> 326                     yielded = next(result)\n        yielded = undefined\n        result = <generator object execute_request>\n    327                     if stack_context._state.contexts is not orig_stack_contexts:\n    328                         yielded = _create_future()\n    329                         yielded.set_exception(\n    330                             stack_context.StackContextInconsistentError(\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\ipykernel\\kernelbase.py in execute_request(self=<ipykernel.ipkernel.IPythonKernel object>, stream=<zmq.eventloop.zmqstream.ZMQStream object>, ident=[b'19361b4e3f56426cb50186b49a66e139'], parent={'buffers': [], 'content': {'allow_stdin': True, 'code': \"model = Pipeline([('tfidf', TfidfVectorizer(stop...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\", 'silent': False, 'stop_on_error': True, 'store_history': True, 'user_expressions': {}}, 'header': {'date': datetime.datetime(2019, 4, 26, 11, 12, 55, 367867, tzinfo=tzutc()), 'msg_id': '5768fbd3da094f2285d1c0938caf2e98', 'msg_type': 'execute_request', 'session': '19361b4e3f56426cb50186b49a66e139', 'username': 'username', 'version': '5.2'}, 'metadata': {}, 'msg_id': '5768fbd3da094f2285d1c0938caf2e98', 'msg_type': 'execute_request', 'parent_header': {}})\n    529             self._publish_execute_input(code, parent, self.execution_count)\n    530 \n    531         reply_content = yield gen.maybe_future(\n    532             self.do_execute(\n    533                 code, silent, store_history,\n--> 534                 user_expressions, allow_stdin,\n        user_expressions = {}\n        allow_stdin = True\n    535             )\n    536         )\n    537 \n    538         # Flush output before sending the reply.\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\tornado\\gen.py in wrapper(*args=(<ipykernel.ipkernel.IPythonKernel object>, \"model = Pipeline([('tfidf', TfidfVectorizer(stop...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\", False, True, {}, True), **kwargs={})\n    321                 # never actually yields, which in turn allows us to\n    322                 # use \"optional\" coroutines in critical path code without\n    323                 # performance penalty for the synchronous case.\n    324                 try:\n    325                     orig_stack_contexts = stack_context._state.contexts\n--> 326                     yielded = next(result)\n        yielded = undefined\n        result = <generator object do_execute>\n    327                     if stack_context._state.contexts is not orig_stack_contexts:\n    328                         yielded = _create_future()\n    329                         yielded.set_exception(\n    330                             stack_context.StackContextInconsistentError(\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\ipykernel\\ipkernel.py in do_execute(self=<ipykernel.ipkernel.IPythonKernel object>, code=\"model = Pipeline([('tfidf', TfidfVectorizer(stop...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\", silent=False, store_history=True, user_expressions={}, allow_stdin=True)\n    289                     res = yield coro_future\n    290             else:\n    291                 # runner isn't already running,\n    292                 # make synchronous call,\n    293                 # letting shell dispatch to loop runners\n--> 294                 res = shell.run_cell(code, store_history=store_history, silent=silent)\n        res = undefined\n        code = \"model = Pipeline([('tfidf', TfidfVectorizer(stop...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\"\n        store_history = True\n        silent = False\n    295         finally:\n    296             self._restore_input()\n    297 \n    298         if res.error_before_exec is not None:\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\ipykernel\\zmqshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, *args=(\"model = Pipeline([('tfidf', TfidfVectorizer(stop...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\",), **kwargs={'silent': False, 'store_history': True})\n    531             )\n    532         self.payload_manager.write_payload(payload)\n    533 \n    534     def run_cell(self, *args, **kwargs):\n    535         self._last_traceback = None\n--> 536         return super(ZMQInteractiveShell, self).run_cell(*args, **kwargs)\n        self.run_cell = <bound method ZMQInteractiveShell.run_cell of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        args = (\"model = Pipeline([('tfidf', TfidfVectorizer(stop...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\",)\n        kwargs = {'silent': False, 'store_history': True}\n    537 \n    538     def _showtraceback(self, etype, evalue, stb):\n    539         # try to preserve ordering of tracebacks and print statements\n    540         sys.stdout.flush()\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=\"model = Pipeline([('tfidf', TfidfVectorizer(stop...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\", store_history=True, silent=False, shell_futures=True)\n   2814         result : :class:`ExecutionResult`\n   2815         \"\"\"\n   2816         result = None\n   2817         try:\n   2818             result = self._run_cell(\n-> 2819                 raw_cell, store_history, silent, shell_futures)\n        raw_cell = \"model = Pipeline([('tfidf', TfidfVectorizer(stop...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\"\n        store_history = True\n        silent = False\n        shell_futures = True\n   2820         finally:\n   2821             self.events.trigger('post_execute')\n   2822             if not silent:\n   2823                 self.events.trigger('post_run_cell', result)\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\IPython\\core\\interactiveshell.py in _run_cell(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=\"model = Pipeline([('tfidf', TfidfVectorizer(stop...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\", store_history=True, silent=False, shell_futures=True)\n   2840             runner = self.loop_runner\n   2841         else:\n   2842             runner = _pseudo_sync_runner\n   2843 \n   2844         try:\n-> 2845             return runner(coro)\n        runner = <function _pseudo_sync_runner>\n        coro = <generator object InteractiveShell.run_cell_async>\n   2846         except BaseException as e:\n   2847             info = ExecutionInfo(raw_cell, store_history, silent, shell_futures)\n   2848             result = ExecutionResult(info)\n   2849             result.error_in_exec = e\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\IPython\\core\\async_helpers.py in _pseudo_sync_runner(coro=<generator object InteractiveShell.run_cell_async>)\n     62 \n     63     Credit to Nathaniel Smith\n     64 \n     65     \"\"\"\n     66     try:\n---> 67         coro.send(None)\n        coro.send = <built-in method send of generator object>\n     68     except StopIteration as exc:\n     69         return exc.value\n     70     else:\n     71         # TODO: do not raise but return an execution result with the right info.\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_cell_async(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, raw_cell=\"model = Pipeline([('tfidf', TfidfVectorizer(stop...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\", store_history=True, silent=False, shell_futures=True)\n   3015                 interactivity = \"none\" if silent else self.ast_node_interactivity\n   3016                 if _run_async:\n   3017                     interactivity = 'async'\n   3018 \n   3019                 has_raised = yield from self.run_ast_nodes(code_ast.body, cell_name,\n-> 3020                        interactivity=interactivity, compiler=compiler, result=result)\n        interactivity = 'last_expr'\n        compiler = <IPython.core.compilerop.CachingCompiler object>\n   3021 \n   3022                 self.last_execution_succeeded = not has_raised\n   3023                 self.last_execution_result = result\n   3024 \n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_ast_nodes(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, nodelist=[<_ast.Assign object>, <_ast.ImportFrom object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Assign object>, <_ast.Expr object>, <_ast.Expr object>], cell_name='<ipython-input-33-185ea2a96147>', interactivity='last', compiler=<IPython.core.compilerop.CachingCompiler object>, result=<ExecutionResult object at 1d396c671d0, executio...rue silent=False shell_futures=True> result=None>)\n   3180                     return True\n   3181             else:\n   3182                 for i, node in enumerate(to_run_exec):\n   3183                     mod = ast.Module([node])\n   3184                     code = compiler(mod, cell_name, \"exec\")\n-> 3185                     if (yield from self.run_code(code, result)):\n        self.run_code = <bound method InteractiveShell.run_code of <ipykernel.zmqshell.ZMQInteractiveShell object>>\n        code = <code object <module> at 0x000001D399CCB9C0, file \"<ipython-input-33-185ea2a96147>\", line 11>\n        result = <ExecutionResult object at 1d396c671d0, executio...rue silent=False shell_futures=True> result=None>\n   3186                         return True\n   3187 \n   3188                 for i, node in enumerate(to_run_interactive):\n   3189                     mod = ast.Interactive([node])\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\IPython\\core\\interactiveshell.py in run_code(self=<ipykernel.zmqshell.ZMQInteractiveShell object>, code_obj=<code object <module> at 0x000001D399CCB9C0, file \"<ipython-input-33-185ea2a96147>\", line 11>, result=<ExecutionResult object at 1d396c671d0, executio...rue silent=False shell_futures=True> result=None>, async_=False)\n   3262                 if async_:\n   3263                     last_expr = (yield from self._async_exec(code_obj, self.user_ns))\n   3264                     code = compile('last_expr', 'fake', \"single\")\n   3265                     exec(code, {'last_expr': last_expr})\n   3266                 else:\n-> 3267                     exec(code_obj, self.user_global_ns, self.user_ns)\n        code_obj = <code object <module> at 0x000001D399CCB9C0, file \"<ipython-input-33-185ea2a96147>\", line 11>\n        self.user_global_ns = {'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'import time\\nimport os\\nimport re\\nfrom wordcloud i...mport pymorphy2\\nmorph = pymorphy2.MorphAnalyzer()', \"\\ndef clear(txt):\\n    tokens=word_tokenize(txt)\\n ...ext'].map(lambda x: clear(str(x)))\\nnum_labels = 7\", 'x_train, x_test,y_train, y_test = train_test_spl...ls, random_state=42, test_size=0.2, shuffle=True)', \"LogReg_pipeline = Pipeline([\\n                ('t...ccuracy_score(y_test[category], prediction), 4)))\", 'import time\\nimport os\\nimport re\\nfrom wordcloud i...mport pymorphy2\\nmorph = pymorphy2.MorphAnalyzer()', 'import time\\nimport os\\nimport re\\nfrom wordcloud i...mport pymorphy2\\nmorph = pymorphy2.MorphAnalyzer()', \"\\ndef clear(txt):\\n    tokens=word_tokenize(txt)\\n ...ext'].map(lambda x: clear(str(x)))\\nnum_labels = 7\", 'import time\\nimport os\\nimport re\\nfrom wordcloud i...mport pymorphy2\\nmorph = pymorphy2.MorphAnalyzer()', 'x_train, x_test,y_train, y_test = train_test_spl...ls, random_state=42, test_size=0.2, shuffle=True)', 'x_train, x_test,y_train, y_test = train_test_spl...ls, random_state=42, test_size=0.2, shuffle=True)', 'LogReg_pipeline = Pipeline([\\n                (\\'t...acy_score(y_test[category], prediction), 4)))\\n\"\"\"', 'LogReg_pipeline = Pipeline([\\n                (\\'t...acy_score(y_test[category], prediction), 4)))\\n\"\"\"', 'LogReg_pipeline = Pipeline([\\n                (\\'t...acy_score(y_test[category], prediction), 4)))\\n\"\"\"', \"\\ndef clear(txt):\\n    tokens=word_tokenize(txt)\\n ...ext'].map(lambda x: clear(str(x)))\\nnum_labels = 7\", 'x_train, x_test,y_train, y_test = train_test_spl...ls, random_state=42, test_size=0.2, shuffle=True)', \"model = Pipeline([('vectorizer', CountVectorizer...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\", \"train, test = train_test_split(data, data.oc[:,'...ls, random_state=42, test_size=0.2, shuffle=True)\", 'train, test = train_test_split(data, data.loc[:,...ls, random_state=42, test_size=0.2, shuffle=True)', 'train, test = train_test_split(data, data.loc[:,...ls, random_state=42, test_size=0.2, shuffle=True)', ...], 'LinearSVC': <class 'sklearn.svm.classes.LinearSVC'>, 'LogReg_pipeline': Pipeline(memory=None,\n     steps=[('tfidf', Tfid...bose=0, warm_start=False),\n          n_jobs=1))]), 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'MultinomialNB': <class 'sklearn.naive_bayes.MultinomialNB'>, 'OneVsRestClassifier': <class 'sklearn.multiclass.OneVsRestClassifier'>, 'Out': {30: '\\nfrom sklearn.model_selection import GridSearchC...curacy_score(y_test[category], prediction), 4)))\\n'}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'STOPWORDS': {'a', 'about', 'above', 'after', 'again', 'against', ...}, ...}\n        self.user_ns = {'GridSearchCV': <class 'sklearn.model_selection._search.GridSearchCV'>, 'In': ['', 'import time\\nimport os\\nimport re\\nfrom wordcloud i...mport pymorphy2\\nmorph = pymorphy2.MorphAnalyzer()', \"\\ndef clear(txt):\\n    tokens=word_tokenize(txt)\\n ...ext'].map(lambda x: clear(str(x)))\\nnum_labels = 7\", 'x_train, x_test,y_train, y_test = train_test_spl...ls, random_state=42, test_size=0.2, shuffle=True)', \"LogReg_pipeline = Pipeline([\\n                ('t...ccuracy_score(y_test[category], prediction), 4)))\", 'import time\\nimport os\\nimport re\\nfrom wordcloud i...mport pymorphy2\\nmorph = pymorphy2.MorphAnalyzer()', 'import time\\nimport os\\nimport re\\nfrom wordcloud i...mport pymorphy2\\nmorph = pymorphy2.MorphAnalyzer()', \"\\ndef clear(txt):\\n    tokens=word_tokenize(txt)\\n ...ext'].map(lambda x: clear(str(x)))\\nnum_labels = 7\", 'import time\\nimport os\\nimport re\\nfrom wordcloud i...mport pymorphy2\\nmorph = pymorphy2.MorphAnalyzer()', 'x_train, x_test,y_train, y_test = train_test_spl...ls, random_state=42, test_size=0.2, shuffle=True)', 'x_train, x_test,y_train, y_test = train_test_spl...ls, random_state=42, test_size=0.2, shuffle=True)', 'LogReg_pipeline = Pipeline([\\n                (\\'t...acy_score(y_test[category], prediction), 4)))\\n\"\"\"', 'LogReg_pipeline = Pipeline([\\n                (\\'t...acy_score(y_test[category], prediction), 4)))\\n\"\"\"', 'LogReg_pipeline = Pipeline([\\n                (\\'t...acy_score(y_test[category], prediction), 4)))\\n\"\"\"', \"\\ndef clear(txt):\\n    tokens=word_tokenize(txt)\\n ...ext'].map(lambda x: clear(str(x)))\\nnum_labels = 7\", 'x_train, x_test,y_train, y_test = train_test_spl...ls, random_state=42, test_size=0.2, shuffle=True)', \"model = Pipeline([('vectorizer', CountVectorizer...f_svm.best_score_)\\nprint(gs_clf_svm.best_params_)\", \"train, test = train_test_split(data, data.oc[:,'...ls, random_state=42, test_size=0.2, shuffle=True)\", 'train, test = train_test_split(data, data.loc[:,...ls, random_state=42, test_size=0.2, shuffle=True)', 'train, test = train_test_split(data, data.loc[:,...ls, random_state=42, test_size=0.2, shuffle=True)', ...], 'LinearSVC': <class 'sklearn.svm.classes.LinearSVC'>, 'LogReg_pipeline': Pipeline(memory=None,\n     steps=[('tfidf', Tfid...bose=0, warm_start=False),\n          n_jobs=1))]), 'LogisticRegression': <class 'sklearn.linear_model.logistic.LogisticRegression'>, 'MultinomialNB': <class 'sklearn.naive_bayes.MultinomialNB'>, 'OneVsRestClassifier': <class 'sklearn.multiclass.OneVsRestClassifier'>, 'Out': {30: '\\nfrom sklearn.model_selection import GridSearchC...curacy_score(y_test[category], prediction), 4)))\\n'}, 'Pipeline': <class 'sklearn.pipeline.Pipeline'>, 'STOPWORDS': {'a', 'about', 'above', 'after', 'again', 'against', ...}, ...}\n   3268             finally:\n   3269                 # Reset our crash handler in place\n   3270                 sys.excepthook = old_excepthook\n   3271         except SystemExit as e:\n\n...........................................................................\nc:\\users\\vasil\\desktop\\parsing_data\\<ipython-input-33-185ea2a96147> in <module>()\n      6     \n      7 from sklearn.model_selection import GridSearchCV\n      8 parameters = {'vectorizer__ngram_range': [(1, 1), (1, 2),(2,2)],\n      9                'tfidf__use_idf': (True, False)}\n     10 gs_clf_svm = GridSearchCV(model, parameters, n_jobs=-1)\n---> 11 gs_clf_svm = gs_clf_svm.fit(data['text'], data.loc[:,'salary':'court_threat'])\n     12 \n     13 print(gs_clf_svm.best_score_)\n     14 print(gs_clf_svm.best_params_)\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\model_selection\\_search.py in fit(self=GridSearchCV(cv=None, error_score='raise',\n     ...ain_score='warn',\n       scoring=None, verbose=0), X=0      ['хотеть', 'получать', 'нормальный', 'зп'..., 'крет...\nName: text, Length: 612, dtype: object, y=     salary  boss  atmosphere  fire_threat  work....0      0.0           0.0\n\n[612 rows x 7 columns], groups=None, **fit_params={})\n    634                                   return_train_score=self.return_train_score,\n    635                                   return_n_test_samples=True,\n    636                                   return_times=True, return_parameters=False,\n    637                                   error_score=self.error_score)\n    638           for parameters, (train, test) in product(candidate_params,\n--> 639                                                    cv.split(X, y, groups)))\n        cv.split = <bound method _BaseKFold.split of KFold(n_splits=3, random_state=None, shuffle=False)>\n        X = 0      ['хотеть', 'получать', 'нормальный', 'зп'..., 'крет...\nName: text, Length: 612, dtype: object\n        y =      salary  boss  atmosphere  fire_threat  work....0      0.0           0.0\n\n[612 rows x 7 columns]\n        groups = None\n    640 \n    641         # if one choose to see train score, \"out\" will contain train score info\n    642         if self.return_train_score:\n    643             (train_score_dicts, test_score_dicts, test_sample_counts, fit_time,\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=Parallel(n_jobs=-1), iterable=<generator object BaseSearchCV.fit.<locals>.<genexpr>>)\n    784             if pre_dispatch == \"all\" or n_jobs == 1:\n    785                 # The iterable was consumed all at once by the above for loop.\n    786                 # No need to wait for async callbacks to trigger to\n    787                 # consumption.\n    788                 self._iterating = False\n--> 789             self.retrieve()\n        self.retrieve = <bound method Parallel.retrieve of Parallel(n_jobs=-1)>\n    790             # Make sure that we get a last message telling us we are done\n    791             elapsed_time = time.time() - self._start_time\n    792             self._print('Done %3i out of %3i | elapsed: %s finished',\n    793                         (len(self._output), len(self._output),\n\n---------------------------------------------------------------------------\nSub-process traceback:\n---------------------------------------------------------------------------\nValueError                                         Fri Apr 26 14:12:57 2019\nPID: 17152        Python 3.6.7: C:\\Users\\vasil\\Anaconda3\\envs\\dp\\python.exe\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in __call__(self=<sklearn.externals.joblib.parallel.BatchedCalls object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        self.items = [(<function _fit_and_score>, (Pipeline(memory=None,\n     steps=[('tfidf', Tfid...l=0.0001,\n     verbose=0),\n          n_jobs=1))]), 0      ['хотеть', 'получать', 'нормальный', 'зп'..., 'крет...\nName: text, Length: 612, dtype: object,      salary  boss  atmosphere  fire_threat  work....0      0.0           0.0\n\n[612 rows x 7 columns], {'score': <function _passthrough_scorer>}, array([204, 205, 206, 207, 208, 209, 210, 211, 2..., 604, 605, 606,\n       607, 608, 609, 610, 611]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...    195, 196, 197, 198, 199, 200, 201, 202, 203]), 0, {'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)}), {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'})]\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py in <listcomp>(.0=<list_iterator object>)\n    126     def __init__(self, iterator_slice):\n    127         self.items = list(iterator_slice)\n    128         self._size = len(self.items)\n    129 \n    130     def __call__(self):\n--> 131         return [func(*args, **kwargs) for func, args, kwargs in self.items]\n        func = <function _fit_and_score>\n        args = (Pipeline(memory=None,\n     steps=[('tfidf', Tfid...l=0.0001,\n     verbose=0),\n          n_jobs=1))]), 0      ['хотеть', 'получать', 'нормальный', 'зп'..., 'крет...\nName: text, Length: 612, dtype: object,      salary  boss  atmosphere  fire_threat  work....0      0.0           0.0\n\n[612 rows x 7 columns], {'score': <function _passthrough_scorer>}, array([204, 205, 206, 207, 208, 209, 210, 211, 2..., 604, 605, 606,\n       607, 608, 609, 610, 611]), array([  0,   1,   2,   3,   4,   5,   6,   7,  ...    195, 196, 197, 198, 199, 200, 201, 202, 203]), 0, {'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)})\n        kwargs = {'error_score': 'raise', 'fit_params': {}, 'return_n_test_samples': True, 'return_parameters': False, 'return_times': True, 'return_train_score': 'warn'}\n    132 \n    133     def __len__(self):\n    134         return self._size\n    135 \n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\model_selection\\_validation.py in _fit_and_score(estimator=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...l=0.0001,\n     verbose=0),\n          n_jobs=1))]), X=0      ['хотеть', 'получать', 'нормальный', 'зп'..., 'крет...\nName: text, Length: 612, dtype: object, y=     salary  boss  atmosphere  fire_threat  work....0      0.0           0.0\n\n[612 rows x 7 columns], scorer={'score': <function _passthrough_scorer>}, train=array([204, 205, 206, 207, 208, 209, 210, 211, 2..., 604, 605, 606,\n       607, 608, 609, 610, 611]), test=array([  0,   1,   2,   3,   4,   5,   6,   7,  ...    195, 196, 197, 198, 199, 200, 201, 202, 203]), verbose=0, parameters={'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)}, fit_params={}, return_train_score='warn', return_parameters=False, return_n_test_samples=True, return_times=True, error_score='raise')\n    439                       for k, v in fit_params.items()])\n    440 \n    441     test_scores = {}\n    442     train_scores = {}\n    443     if parameters is not None:\n--> 444         estimator.set_params(**parameters)\n        estimator.set_params = <bound method Pipeline.set_params of Pipeline(me...=0.0001,\n     verbose=0),\n          n_jobs=1))])>\n        parameters = {'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)}\n    445 \n    446     start_time = time.time()\n    447 \n    448     X_train, y_train = _safe_split(estimator, X, y, train)\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\pipeline.py in set_params(self=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...l=0.0001,\n     verbose=0),\n          n_jobs=1))]), **kwargs={'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)})\n    137 \n    138         Returns\n    139         -------\n    140         self\n    141         \"\"\"\n--> 142         self._set_params('steps', **kwargs)\n        self._set_params = <bound method _BaseComposition._set_params of Pi...=0.0001,\n     verbose=0),\n          n_jobs=1))])>\n        kwargs = {'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)}\n    143         return self\n    144 \n    145     def _validate_steps(self):\n    146         names, estimators = zip(*self.steps)\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\utils\\metaestimators.py in _set_params(self=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...l=0.0001,\n     verbose=0),\n          n_jobs=1))]), attr='steps', **params={'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)})\n     44         names, _ = zip(*getattr(self, attr))\n     45         for name in list(six.iterkeys(params)):\n     46             if '__' not in name and name in names:\n     47                 self._replace_estimator(attr, name, params.pop(name))\n     48         # 3. Step parameters and other initilisation arguments\n---> 49         super(_BaseComposition, self).set_params(**params)\n        self.set_params = <bound method Pipeline.set_params of Pipeline(me...=0.0001,\n     verbose=0),\n          n_jobs=1))])>\n        params = {'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)}\n     50         return self\n     51 \n     52     def _replace_estimator(self, attr, name, new_val):\n     53         # assumes `name` is a valid estimator name\n\n...........................................................................\nC:\\Users\\vasil\\Anaconda3\\envs\\dp\\lib\\site-packages\\sklearn\\base.py in set_params(self=Pipeline(memory=None,\n     steps=[('tfidf', Tfid...l=0.0001,\n     verbose=0),\n          n_jobs=1))]), **params={'tfidf__use_idf': True, 'vectorizer__ngram_range': (1, 1)})\n    269             key, delim, sub_key = key.partition('__')\n    270             if key not in valid_params:\n    271                 raise ValueError('Invalid parameter %s for estimator %s. '\n    272                                  'Check the list of available parameters '\n    273                                  'with `estimator.get_params().keys()`.' %\n--> 274                                  (key, self))\n        key = 'vectorizer'\n        self = Pipeline(memory=None,\n     steps=[('tfidf', Tfid...l=0.0001,\n     verbose=0),\n          n_jobs=1))])\n    275 \n    276             if delim:\n    277                 nested_params[key][sub_key] = value\n    278             else:\n\nValueError: Invalid parameter vectorizer for estimator Pipeline(memory=None,\n     steps=[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n ...lti_class='ovr', penalty='l2', random_state=None, tol=0.0001,\n     verbose=0),\n          n_jobs=1))]). Check the list of available parameters with `estimator.get_params().keys()`.\n___________________________________________________________________________"
     ]
    }
   ],
   "source": [
    "model = Pipeline([('tfidf', TfidfVectorizer(stop_words=stopwords.words('russian'))),\n",
    " ('clf', OneVsRestClassifier(LinearSVC(class_weight='balanced')))])\n",
    "#the class_weight=\"balanced\" option tries to remove the biasedness of model towards majority sample\n",
    "\n",
    "#paramater selection\n",
    "    \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vectorizer__ngram_range': [(1, 1), (1, 2),(2,2)],\n",
    "               'tfidf__use_idf': (True, False)}\n",
    "gs_clf_svm = GridSearchCV(model, parameters, n_jobs=-1)\n",
    "gs_clf_svm = gs_clf_svm.fit(data['text'], data.loc[:,'salary':'court_threat'])\n",
    "\n",
    "print(gs_clf_svm.best_score_)\n",
    "print(gs_clf_svm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom sklearn.model_selection import GridSearchCV\\nparameters = {'vectorizer__ngram_range': [(1, 1), (1, 2),(2,2)],\\n               'tfidf__use_idf': (True, False)}\\n\\ngs_log_reg = GridSearchCV(LogReg_pipeline, parameters, n_jobs=-1)\\ngs_log_reg = gs_log_reg.fit(data['text'], data.loc[:,'salary':'court_threat'])\\nprint(gs_log_reg.best_score_)\\nprint(gs_log_reg.best_params_)\\n\\nfor category in labels:\\n    print('Категория: {}'.format(category))\\n    LogReg_pipeline.fit(x_train, y_train[category])\\n    prediction = LogReg_pipeline.predict(x_test)\\n    print('Точность на тестовых данных: {}'.format(round(accuracy_score(y_test[category], prediction), 4)))\\n\""
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#LogReg_pipeline = Pipeline([\n",
    "  #              ('tfidf', TfidfVectorizer(stop_words=stopwords.words('russian'))),\n",
    "  #              ('clf', OneVsRestClassifier(LogisticRegression(solver='sag'), n_jobs=1)),\n",
    "   #         ])\n",
    "SVC_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stopwords.words('russian'))),\n",
    "                ('clf', OneVsRestClassifier(LinearSVC(), n_jobs=1)),\n",
    "            ])\n",
    "NB_pipeline = Pipeline([\n",
    "                ('tfidf', TfidfVectorizer(stop_words=stopwords.words('russian'))),\n",
    "                ('clf', OneVsRestClassifier(MultinomialNB(\n",
    "                    fit_prior=True, class_prior=None))),\n",
    "            ])\n",
    "\"\"\"\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {'vectorizer__ngram_range': [(1, 1), (1, 2),(2,2)],\n",
    "               'tfidf__use_idf': (True, False)}\n",
    "\n",
    "gs_log_reg = GridSearchCV(LogReg_pipeline, parameters, n_jobs=-1)\n",
    "gs_log_reg = gs_log_reg.fit(data['text'], data.loc[:,'salary':'court_threat'])\n",
    "print(gs_log_reg.best_score_)\n",
    "print(gs_log_reg.best_params_)\n",
    "\n",
    "for category in labels:\n",
    "    print('Категория: {}'.format(category))\n",
    "    LogReg_pipeline.fit(x_train, y_train[category])\n",
    "    prediction = LogReg_pipeline.predict(x_test)\n",
    "    print('Точность на тестовых данных: {}'.format(round(accuracy_score(y_test[category], prediction), 4)))\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Категория: salary\n",
      "Точность на тестовых данных: 0.8049\n",
      "Категория: boss\n",
      "Точность на тестовых данных: 0.7642\n",
      "Категория: atmosphere\n",
      "Точность на тестовых данных: 0.7154\n",
      "Категория: fire_threat\n",
      "Точность на тестовых данных: 0.9187\n",
      "Категория: work_cond\n",
      "Точность на тестовых данных: 0.7236\n",
      "Категория: neutral\n",
      "Точность на тестовых данных: 0.8049\n",
      "Категория: court_threat\n",
      "Точность на тестовых данных: 0.935\n"
     ]
    }
   ],
   "source": [
    "for topic in labels:\n",
    "    print('Категория: {}'.format(topic))\n",
    "    NB_pipeline.fit(x_train, y_train[topic])\n",
    "    prediction = NB_pipeline.predict(x_test)\n",
    "    print('Точность на тестовых данных: {}'.format(\n",
    "    round(accuracy_score(y_test[topic], prediction), 4)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
